{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNc/REutVVuGpP7JBoGCe86"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbwNeuRn4NqS","executionInfo":{"status":"ok","timestamp":1702205659677,"user_tz":-60,"elapsed":74594,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"8c2cf697-d184-4d02-d305-f02db8503356"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n","Collecting farasa\n","  Downloading Farasa-0.0.1-py2.py3-none-any.whl (12.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: farasa\n","Successfully installed farasa-0.0.1\n","\u001b[31mERROR: Could not find a version that satisfies the requirement FarasaSegmenter (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for FarasaSegmenter\u001b[0m\u001b[31m\n","\u001b[0mCollecting pyarabic\n","  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n","Installing collected packages: pyarabic\n","Successfully installed pyarabic-0.6.15\n","\u001b[31mERROR: Could not find a version that satisfies the requirement arabicstopwords (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for arabicstopwords\u001b[0m\u001b[31m\n","\u001b[0mCollecting stop-words\n","  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: stop-words\n","  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32895 sha256=d3e088692aa14a0f9dc18a825a85cfd5113fbba6b914dec200463fb1c9d770a1\n","  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n","Successfully built stop-words\n","Installing collected packages: stop-words\n","Successfully installed stop-words-2018.7.23\n","Collecting tashaphyne\n","  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarabic in /usr/local/lib/python3.10/dist-packages (from tashaphyne) (0.6.15)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic->tashaphyne) (1.16.0)\n","Installing collected packages: tashaphyne\n","Successfully installed tashaphyne-0.3.6\n","Collecting Arabic-Stopwords\n","  Downloading Arabic_Stopwords-0.4.3-py3-none-any.whl (360 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from Arabic-Stopwords) (0.6.15)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic>=0.6.2->Arabic-Stopwords) (1.16.0)\n","Installing collected packages: Arabic-Stopwords\n","Successfully installed Arabic-Stopwords-0.4.3\n","Collecting farasapy\n","  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.66.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2023.11.17)\n","Installing collected packages: farasapy\n","Successfully installed farasapy-0.0.14\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["\n","!pip install gensim\n","!pip install farasa\n","!pip install FarasaSegmenter\n","!pip install pyarabic\n","!pip install arabicstopwords\n","!pip install stop-words\n","!pip install tashaphyne\n","!pip install Arabic-Stopwords\n","!pip install farasapy\n","!pip install nltk\n","import nltk\n","\n","# Download the 'punkt' resource\n","nltk.download('punkt')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","\n","nltk.download('stopwords')\n","nltk.download(\"punkt\")\n","from nltk.corpus import stopwords\n","from nltk.stem.isri import ISRIStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from farasa.segmenter import FarasaSegmenter\n","from farasa.stemmer import FarasaStemmer\n","from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel\n","from pyarabic import trans\n","import arabicstopwords.arabicstopwords as stp\n","import string\n","from itertools import groupby\n","import re\n","from gensim import corpora, models\n","\n","\n","import http.client\n","import json\n","import requests\n","from tashaphyne.stemming import ArabicLightStemmer\n","\n","#Strip vowels from a text, include Shadda\n","from pyarabic.araby import strip_tashkeel\n","#Standardize the Hamzat into one form of hamza, replace Madda by hamza and alef.\n","import pyarabic.araby as araby"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jabxIg__4RF0","executionInfo":{"status":"ok","timestamp":1702205772431,"user_tz":-60,"elapsed":1858,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"6fb4101b-e2b9-4866-d65e-e5ed709cb8b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZTytsT314Whe"}},{"cell_type":"code","source":["class preprocessing:\n","\n","  arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n","  english_punctuations = string.punctuation\n","  punctuations_list = arabic_punctuations + english_punctuations\n","  stop_words = stopwords.words('arabic')\n","  stop_words.append(['ان','او','الى','اي','اما','شي','الا','يا'])\n","  ArListem = ArabicLightStemmer()\n","\n","  def __init__(self, stemming = None, del_punctiation = True, del_emoji = True, del_consecutive = True, del_stopword = True):\n","    self.stemming = stemming\n","    self.del_punctiation = del_punctiation\n","    self.del_emoji = del_emoji\n","    self.del_consecutive = del_consecutive\n","    self.del_stopword = del_stopword\n","    self.indexLoop = 0\n","\n","\n","  #Flatten\n","  def __flatten(self, t):\n","    return [item for sublist in t for item in sublist]\n","\n","  #remove punctiation\n","  def __remove_punctuations(self, text):\n","    translator = str.maketrans('', '', self.punctuations_list)\n","    return text.translate(translator)\n","\n","  def del_punc(self, elements):\n","    if self.del_punctiation == True:\n","      wordsForTf = []\n","      for element in elements:\n","        word = self.__remove_punctuations(element)\n","        if word != '':\n","          wordsForTf.append(word)\n","      return wordsForTf\n","    else:\n","      return elements\n","\n","  #remove consecutive\n","  def remove_consecutive(self, text:list):\n","    if self.del_consecutive == True:\n","      comment=list()\n","      for word in text:\n","          if len(word)==1:\n","              continue\n","          groups = groupby(word)\n","          new_text=''\n","          result = [(label, sum(1 for _ in group)) for label, group in groups]\n","          for letter in result:\n","              if letter[1]>2:\n","                  new_text+=letter[0]\n","              else:\n","                  if letter[1]==2:\n","                      for i in range(2):\n","                          new_text+=letter[0]\n","                  else:\n","                      new_text+=letter[0]\n","          comment.append(new_text)\n","      return comment\n","    else:\n","      return text\n","\n","  #remove Emoji\n","  def deEmoji(self, text):\n","    if self.del_emoji == True:\n","      regrex_pattern = re.compile(pattern = \"[\"\n","          u\"\\U0001F600-\\U0001F64F\"\n","          u\"\\U0001F300-\\U0001F5FF\"\n","          u\"\\U0001F680-\\U0001F6FF\"\n","          u\"\\U0001F1E0-\\U0001F1FF\"\n","                            \"]+\", flags = re.UNICODE)\n","      return regrex_pattern.sub(r'',text)\n","    else:\n","      return text\n","\n","  #remove Stop Words\n","  def del_StopWords(self, wordss,hamza_normalize=False):\n","\n","    if self.del_stopword == True:\n","      if hamza_normalize == True:\n","        stop_words_normalize=[self.normalize_hamza(word) for word in self.stop_words]\n","        return [ [word for word in comment if word not in stop_words_normalize] for comment in wordss ]\n","\n","      wordsWstop = []\n","      wordsWstopT = []\n","      for liste in wordss:\n","        wordsWstop.append([element for element in liste if stp.is_stop(element) == False])\n","\n","      for liste in wordsWstop:\n","        wordsWstopT.append([element for element in liste if element not in self.stop_words])\n","      return wordsWstopT\n","    else:\n","      return wordss\n","\n","\n","  def token(self, textSEmoji):\n","    words = []\n","    for comment in textSEmoji:\n","      words.append(tokenize(comment, conditions=is_arabicrange, morphs=strip_tashkeel))\n","\n","    return words\n","\n","\n","  def stemLineFarasa(self, tokens):\n","    text = \" \".join(tokens)\n","    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n","    payload = {\"text\": text}\n","    headers = {\"content-type\": \"application/json\", \"cache-control\": \"no-cache\"}\n","    conn.request(\"POST\", \"/msa/webapi/lemma\", json.dumps(payload), headers, encode_chunked=False)\n","    res = conn.getresponse()\n","    data = res.read()\n","    result = json.loads(data.decode(\"utf-8\"))\n","    print(self.indexLoop)\n","    self.indexLoop += 1\n","    return result[\"result\"]\n","\n","\n","  def stemLineISRI(self, tokens):\n","    isri = ISRIStemmer()\n","    StemISRI = [isri.stem(text) for text in tokens]\n","    return StemISRI\n","\n","  def stemLineLight(self, tokens):\n","    StemLight = []\n","    for token in tokens:\n","      stem = self.ArListem.light_stem(token)\n","      StemLight.append(self.ArListem.get_stem())\n","    return StemLight\n","\n","\n","  def root(self, tokens):\n","    racine = []\n","    for token in tokens:\n","      stem = self.ArListem.light_stem(token)\n","      racine.append(self.ArListem.get_root())\n","    return racine\n","\n","  def uniques(self,data):\n","        #finding unique\n","    unique = []\n","    for doc in data:\n","      for word in doc:\n","        if word not in unique:\n","            unique.append(word)\n","    return len(unique)\n","\n","\n","  def remove_length2(self,data:list,length=2):\n","    return [[word for word in comment if len(word) >length ] for comment in data]\n","\n","\n","  def normalize_hamza(self,data,method=\"tasheel\"):\n","    if type(data)==str:\n","      return araby.normalize_hamza(data, method=method)\n","\n","    return [araby.normalize_hamza(comment, method=method) for comment in data]\n","\n","  def preprocess(self, data):\n","    #list of text data\n","    textSEmoji = np.array([self.deEmoji(text) for text in data])\n","\n","    wordss = []\n","    for word in self.token(textSEmoji):\n","      # word is vector of words\n","      wordss.append(self.remove_consecutive(word))\n","\n","    wordss=self.del_StopWords(wordss)\n","\n","    if self.stemming == 'farasa':\n","      stemmi = [self.stemLineFarasa(tokens) for tokens in wordss]\n","    elif self.stemming == 'isri':\n","      stemmi = [self.stemLineISRI(tokens) for tokens in wordss]\n","    elif self.stemming == 'light':\n","      stemmi = [self.stemLineLight(tokens) for tokens in wordss]\n","    elif self.stemming == 'root':\n","      stemmi = [self.root(tokens) for tokens in wordss]\n","    else:\n","      stemmi = wordss\n","\n","    wordsWstop = self.del_StopWords(stemmi)\n","\n","    wordsForTf = np.array([self.del_punc(word) for word in wordsWstop])\n","\n","    return wordsForTf"],"metadata":{"id":"oy2S55IK4W9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load the data"],"metadata":{"id":"1qd7MYZ94bKC"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv('/content/train.csv')\n","IDs=data['ID'].to_numpy()\n","comments = data['comment'].to_numpy()\n","labeles =  data['label'].to_numpy()\n","\n","\n","\n","data_test = pd.read_csv('test_stage1.csv')\n","IDs_test=data_test['ID'].to_numpy()\n","comments_test = data_test['comment'].to_numpy()\n","data_test.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"jMnt86Xm4a_T","executionInfo":{"status":"ok","timestamp":1702205780084,"user_tz":-60,"elapsed":390,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"6347bc38-91b7-47da-8031-4b0291536c5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ID                                            comment\n","0   1  التلقيح نعمة لانه فابور,الناس في مصر را كتخلص ...\n","1   2             أثبت التلقيح أهميته في الحد من الوفيات\n","2   3  انا ملقح جوج مرات والحمد لله عندي المناعة ديال...\n","3   4  كنــا متأكديــن من أن جلالـة الملــك سيجعل الت...\n","4   5  شعب أناني مشبع بثقافة الخرافة والمؤامرة… بنادم..."],"text/html":["\n","  <div id=\"df-3e339f6d-80b9-4781-b1ba-969792d8f0b9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>التلقيح نعمة لانه فابور,الناس في مصر را كتخلص ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>أثبت التلقيح أهميته في الحد من الوفيات</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>انا ملقح جوج مرات والحمد لله عندي المناعة ديال...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>كنــا متأكديــن من أن جلالـة الملــك سيجعل الت...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>شعب أناني مشبع بثقافة الخرافة والمؤامرة… بنادم...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e339f6d-80b9-4781-b1ba-969792d8f0b9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3e339f6d-80b9-4781-b1ba-969792d8f0b9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3e339f6d-80b9-4781-b1ba-969792d8f0b9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-046488eb-4d7d-458c-b3c6-f8bbb14e87af\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-046488eb-4d7d-458c-b3c6-f8bbb14e87af')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-046488eb-4d7d-458c-b3c6-f8bbb14e87af button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import pandas as pd\n","from textblob import TextBlob  # For sentiment analysis\n","\n","# Assuming you have a DataFrame called 'data' with a column 'comment'\n","# Replace 'data' with your actual DataFrame name\n","\n","# Filter out short comments\n","data = data[data['comment'].apply(lambda x: len(str(x).split()) > 3)]\n","\n","# Remove duplicate comments\n","data = data.drop_duplicates(subset=['comment'])\n","\n","# Use sentiment analysis to filter out extremely negative or positive comments\n","data['sentiment'] = data['comment'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n","data = data[data['sentiment'].between(-0.5, 0.5)]\n","\n","# Drop the temporary 'sentiment' column\n","data = data.drop(columns=['sentiment'])\n","\n","# Optionally, you can save the cleaned data to a new CSV file\n","data.to_csv('cleaned_data.csv', index=False)"],"metadata":{"id":"QW4fXudGklqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv('/content/cleaned_data.csv')\n","IDs=data['ID'].to_numpy()\n","comments = data['comment'].to_numpy()\n","labeles =  data['label'].to_numpy()"],"metadata":{"id":"NkV-wKLektYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prepro = preprocessing(stemming='light')\n","textPre = prepro.preprocess(comments)\n","\n","textPre_dev = prepro.preprocess(comments_test)\n","\n","train_input_data=[' '.join(doc) for doc in textPre]\n","test_input_data=[' '.join(doc) for doc in textPre_dev]\n","\n","print(len(labeles))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxA8mzw14oGl","executionInfo":{"status":"ok","timestamp":1702206550509,"user_tz":-60,"elapsed":12229,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"48e2e5c8-762d-4f96-87cf-d8a0defb61ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-43071c50821a>:182: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  wordsForTf = np.array([self.del_punc(word) for word in wordsWstop])\n"]},{"output_type":"stream","name":"stdout","text":["1898\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Flatten, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import load_model\n","import joblib\n","\n","# Assuming you have already loaded your data\n","# ...\n","\n","# Perform train-test split on the training data\n","train_input_data, validation_input_data, labels_train, labels_validation = train_test_split(\n","    train_input_data, labeles, test_size=0.2, random_state=42\n",")\n","\n","# Tokenize and pad the training data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_input_data)\n","train_sequences = tokenizer.texts_to_sequences(train_input_data)\n","validation_sequences = tokenizer.texts_to_sequences(validation_input_data)\n","\n","max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in validation_sequences))\n","train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n","validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n","\n","# Define and train the embedding model with different optimizers\n","optimizers = ['adam', 'sgd']  # Add more optimizers if needed\n","\n","for optimizer_name in optimizers:\n","    print(f\"Training with optimizer: {optimizer_name}\")\n","\n","    # Create a new model for each optimizer\n","    model = Sequential([\n","        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length),\n","        Flatten(),\n","        Dense(64, activation='relu'),\n","        Dense(1, activation='sigmoid')\n","    ])\n","\n","    # Compile the model with the selected optimizer\n","    if optimizer_name.lower() == 'adam':\n","        optimizer = Adam()\n","    elif optimizer_name.lower() == 'sgd':\n","        optimizer = SGD()\n","    else:\n","        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n","\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model and validate on the validation set\n","    model.fit(\n","        train_padded, labels_train,\n","        epochs=5,\n","        validation_data=(validation_padded, labels_validation)\n","    )\n","\n","    # Save the trained model\n","    model.save(f'/content/your_trained_model_{optimizer_name}.h5')\n","\n","    # Extract features using the trained model\n","    embedding_model = Sequential([\n","        model.layers[0]  # Assuming the embedding layer is the first layer in your model\n","    ])\n","\n","    train_features = embedding_model.predict(train_padded)\n","    train_features_flattened = train_features.reshape(train_features.shape[0], -1)\n","\n","    # Train the SVM model\n","    svm_model = SVC()\n","    svm_model.fit(train_features_flattened, labels_train)\n","\n","    # Save the SVM model\n","    joblib.dump(svm_model, f'/content/svm_model_{optimizer_name}.joblib')\n","\n","    # Continue with the rest of your code...\n","\n","# Optionally, print the final validation accuracy after training\n","final_validation_accuracy = history.history['val_accuracy'][-1]\n","print(f'Final Validation Accuracy: {final_validation_accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuygdczW7-HF","executionInfo":{"status":"ok","timestamp":1702206530214,"user_tz":-60,"elapsed":72496,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"a5a2f98a-470c-47d1-e652-34a62ed666d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training with optimizer: adam\n","Epoch 1/5\n","48/48 [==============================] - 3s 42ms/step - loss: 0.6763 - accuracy: 0.5817 - val_loss: 0.6352 - val_accuracy: 0.6605\n","Epoch 2/5\n","48/48 [==============================] - 1s 27ms/step - loss: 0.4887 - accuracy: 0.7958 - val_loss: 0.5323 - val_accuracy: 0.7263\n","Epoch 3/5\n","48/48 [==============================] - 1s 27ms/step - loss: 0.1326 - accuracy: 0.9651 - val_loss: 0.5022 - val_accuracy: 0.7553\n","Epoch 4/5\n","48/48 [==============================] - 1s 26ms/step - loss: 0.0244 - accuracy: 0.9974 - val_loss: 0.5352 - val_accuracy: 0.7421\n","Epoch 5/5\n","48/48 [==============================] - 2s 38ms/step - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.5684 - val_accuracy: 0.7447\n","48/48 [==============================] - 0s 4ms/step\n","Training with optimizer: sgd\n","Epoch 1/5\n","48/48 [==============================] - 3s 40ms/step - loss: 0.6930 - accuracy: 0.5132 - val_loss: 0.6908 - val_accuracy: 0.5684\n","Epoch 2/5\n","48/48 [==============================] - 1s 27ms/step - loss: 0.6904 - accuracy: 0.5922 - val_loss: 0.6879 - val_accuracy: 0.6263\n","Epoch 3/5\n","48/48 [==============================] - 1s 27ms/step - loss: 0.6884 - accuracy: 0.6014 - val_loss: 0.6852 - val_accuracy: 0.6263\n","Epoch 4/5\n","48/48 [==============================] - 1s 23ms/step - loss: 0.6868 - accuracy: 0.5995 - val_loss: 0.6833 - val_accuracy: 0.6316\n","Epoch 5/5\n","48/48 [==============================] - 1s 29ms/step - loss: 0.6850 - accuracy: 0.5995 - val_loss: 0.6821 - val_accuracy: 0.6395\n"," 1/48 [..............................] - ETA: 2s"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 0s 3ms/step\n","Final Validation Accuracy: 76.58%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.models import load_model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the trained embedding model\n","embedding_model = load_model('/content/your_trained_model_adam.h5')\n","\n","\n","# Tokenize and pad the test data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(test_input_data)\n","test_sequences = tokenizer.texts_to_sequences(test_input_data)\n","test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","\n","# Extract features using the trained embedding model\n","test_features = embedding_model.predict(test_padded)\n","test_features_flattened = test_features.reshape(test_features.shape[0], -1)\n","\n","# Load the labels or ground truth for the test data\n","labels_test = data_test['label'].to_numpy()\n","\n","# Train a Naive Bayes model\n","nb_model = MultinomialNB()\n","nb_model.fit(test_features_flattened, labels_test)\n","\n","# Optionally, you can print the accuracy on the training set (although it's typically more meaningful on a separate test set)\n","train_predictions = nb_model.predict(test_features_flattened)\n","train_accuracy = accuracy_score(labels_test, train_predictions)\n","print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n","\n"],"metadata":{"id":"bxevJ5AxaWL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" import pandas as pd\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.models import load_model\n","import joblib\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the saved models\n","optimizers = ['adam', 'sgd']\n","\n","for optimizer_name in optimizers:\n","    # Load the trained model\n","    model = load_model(f'/content/your_trained_model_adam.h5')\n","\n","    # Load the SVM model\n","    svm_model = joblib.load(f'/content/svm_model_{optimizer_name}.joblib')\n","\n","    # Load the Naive Bayes model\n","    nb_model = joblib.load(f'/content/nb_model_{optimizer_name}.joblib')\n","\n","    # Load the test data\n","    data_test = pd.read_excel('/content/Dataset.xlsx')\n","    comments_test = data_test['comment'].to_numpy()\n","\n","    # Preprocess the test data\n","    prepro = preprocessing(stemming='light')\n","    textPre_test = prepro.preprocess(comments_test)\n","    test_input_data = [' '.join(doc) for doc in textPre_test]\n","\n","    # Tokenize and pad the test data\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(train_input_data)\n","    test_sequences = tokenizer.texts_to_sequences(test_input_data)\n","    test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","\n","    # Extract features using the trained model\n","    embedding_model = Sequential([model.layers[0]])\n","\n","    test_features = embedding_model.predict(test_padded)\n","    test_features_flattened = test_features.reshape(test_features.shape[0], -1)\n","\n","    # Predict using SVM\n","    svm_predictions = svm_model.predict(test_features_flattened)\n","\n","    # Predict using Naive Bayes\n","    nb_predictions = nb_model.predict(test_features_flattened)\n","\n","    # Print SVM accuracy\n","    svm_accuracy = accuracy_score(labels_test, svm_predictions)\n","    print(f'SVM Accuracy with {optimizer_name} optimizer: {svm_accuracy * 100:.2f}%')\n","\n","    # Print Naive Bayes accuracy\n","    nb_accuracy = accuracy_score(labels_test, nb_predictions)\n","    print(f'Naive Bayes Accuracy with {optimizer_name} optimizer: {nb_accuracy * 100:.2f}%')\n"],"metadata":{"id":"v5Kf25MCaFhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming the previous code has been executed\n","\n","# Load the test data\n","excel_file_path = '/content/2 vaccination train set.xlsx'\n","data_test = pd.read_excel(excel_file_path)\n","comments_test = data_test['comment'].to_numpy()\n","labels_test = data_test['labels'].to_numpy()\n","\n","# Preprocess the test data\n","textPre_test = prepro.preprocess(comments_test)\n","test_input_data = [' '.join(doc) for doc in textPre_test]\n","\n","# Tokenize and pad the test data\n","test_sequences = tokenizer.texts_to_sequences(test_input_data)\n","test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n","\n","# Evaluate the models on the test set\n","for optimizer_name in optimizers:\n","    print(f\"Evaluating model with optimizer: {optimizer_name}\")\n","\n","    # Load the trained embedding model\n","    trained_model = load_model(f'/content/your_trained_model_adam.h5')\n","\n","    # Extract features using the trained embedding model\n","    embedding_model = Sequential([\n","        trained_model.layers[0]  # Assuming the embedding layer is the first layer in your model\n","    ])\n","\n","    test_features = embedding_model.predict(test_padded)\n","    test_features_flattened = test_features.reshape(test_features.shape[0], -1)\n","\n","    # Load the trained SVM model\n","    loaded_svm_model = joblib.load(f'/content/svm_model_adam.joblib')\n","\n","    # Predict labels for the test data using SVM\n","    test_predictions = loaded_svm_model.predict(test_features_flattened)\n","\n","    # Calculate and print accuracy\n","    test_accuracy = accuracy_score(labels_test, test_predictions)\n","    print(f'Test Accuracy with {optimizer_name} optimizer: {test_accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCC0ArdfXUdx","executionInfo":{"status":"ok","timestamp":1702206248998,"user_tz":-60,"elapsed":50902,"user":{"displayName":"Youssef Azami","userId":"12577427614956967667"}},"outputId":"303b6036-1fc3-4164-c506-b83c622c6bf8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-43071c50821a>:182: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  wordsForTf = np.array([self.del_punc(word) for word in wordsWstop])\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating model with optimizer: adam\n","45/45 [==============================] - 0s 2ms/step\n","Test Accuracy with adam optimizer: 94.38%\n","Evaluating model with optimizer: sgd\n","45/45 [==============================] - 0s 2ms/step\n","Test Accuracy with sgd optimizer: 94.38%\n"]}]}]}